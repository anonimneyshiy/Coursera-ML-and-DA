{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import impute, preprocessing\n",
    "from sklearn import pipeline, compose\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import linear_model, ensemble\n",
    "import catboost\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цель работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе построим baseline-модели разного типа и сделаем предположения о том, какие подходы являются наиболее перспективными, чтобы в дальнейшим оценить какой прирост в качества дают различные преобразования, усложнения, оптимизации. \n",
    "\n",
    "На разработку baseline-модели не должно уходить много времени, поэтому будем использовать простые решения. В предобработке данных ограничимся удалением константных признаков, заполнением пропусков среднимим значениями у вещественных и самыми популярными значениями у категориальных признаков. Далее сделаем стандартизацию и кодируем категориальные признаки.\n",
    "\n",
    "В качестве baseline-моделей будем использовать следующие:\n",
    "\n",
    " - Логистическую регрессию\n",
    " - Случайный лес\n",
    " - Градиентный бустинг\n",
    " \n",
    "Оценку качества модели будем производить на основе кросс-валидации. Будем использовать stratified k-fold с разбиением на 4 фолда. В качестве основной метрики оценки качества модели будем использовать ROC-AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импортируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt')\n",
    "labels = pd.read_csv('orange_small_churn_labels.txt', \n",
    "                     header = None, names = ['churn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на несколько строк данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 230)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3052.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>vr93T2a</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>xwM2aC7IdeMC0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>6hQ9lNX</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>55YFVY9</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>zCkv</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>e4lqvY0</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>686.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>MAz3HNj</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9  Var10  ...  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN  3052.0   NaN   NaN   NaN    NaN  ...   \n",
       "1   NaN   NaN   NaN   NaN   NaN  1813.0   7.0   NaN   NaN    NaN  ...   \n",
       "2   NaN   NaN   NaN   NaN   NaN  1953.0   7.0   NaN   NaN    NaN  ...   \n",
       "3   NaN   NaN   NaN   NaN   NaN  1533.0   7.0   NaN   NaN    NaN  ...   \n",
       "4   NaN   NaN   NaN   NaN   NaN   686.0   7.0   NaN   NaN    NaN  ...   \n",
       "\n",
       "    Var221   Var222      Var223  Var224  Var225  Var226   Var227  \\\n",
       "0  Al6ZaUT  vr93T2a  LM8l689qOp     NaN     NaN    fKCe  02N6s8f   \n",
       "1     oslk  6hQ9lNX  LM8l689qOp     NaN    ELof    xb3V     RAYp   \n",
       "2     zCkv  catzS2D  LM8l689qOp     NaN     NaN    FSa2     ZI9m   \n",
       "3     oslk  e4lqvY0  LM8l689qOp     NaN     NaN    xb3V     RAYp   \n",
       "4     oslk  MAz3HNj  LM8l689qOp     NaN     NaN    WqMG     RAYp   \n",
       "\n",
       "          Var228  Var229  Var230  \n",
       "0  xwM2aC7IdeMC0     NaN     NaN  \n",
       "1        55YFVY9    mj86     NaN  \n",
       "2  ib5G6X1eUxUn6    mj86     NaN  \n",
       "3  F2FyR07IdsN7I     NaN     NaN  \n",
       "4  F2FyR07IdsN7I     NaN     NaN  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим признаки на вещественные и категориальные. Признаки, которые имеют одно уникальное значение не будем использовать, т к константные признаки не имеют смысла при построении модели. \n",
    "\n",
    "Также не будем использовать категориальные признаки, вариация значений которых больше 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_border(data, low, hight, num = True):\n",
    "    all_features = [''.join(('Var', str(i))) for i in range(low, hight)]\n",
    "    result_features = []\n",
    "    for var in all_features:\n",
    "        size_feature = data[var].unique().shape[0]\n",
    "        if num:\n",
    "            if size_feature >= 2:\n",
    "                result_features.append(var)\n",
    "        else:\n",
    "            if size_feature >= 2 and size_feature < 300:\n",
    "                result_features.append(var)\n",
    "    return result_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = feature_border(data, 1, 191)\n",
    "cat_features = feature_border(data, 191, 231, num = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_feaures size: 174\n",
      "cat_feaures size: 28\n"
     ]
    }
   ],
   "source": [
    "print('num_feaures size: {}'.format(len(num_features)))\n",
    "print('cat_feaures size: {}'.format(len(cat_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оставим отложенную выборку для тестирования качества модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train size: (32000, 230)\n",
      "labels_train size: (32000, 1)\n",
      "\n",
      "data_test size: (8000, 230)\n",
      "labels_test size: (8000, 1)\n"
     ]
    }
   ],
   "source": [
    "(data_train, data_test, \n",
    " y_train, y_test) = model_selection.train_test_split(data, labels, \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state = 1)\n",
    "\n",
    "\n",
    "print('data_train size: {}'.format(data_train.shape))\n",
    "print('labels_train size: {}'.format(y_train.shape))\n",
    "print('')\n",
    "print('data_test size: {}'.format(data_test.shape))\n",
    "print('labels_test size: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Заполним пропуски в данных:\n",
    "Для вещественных признаков пропуски заполним средним значением по столбцу, а для категориальных константным значением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preprocessing = pipeline.Pipeline(steps = [\n",
    "    ('num', impute.SimpleImputer())\n",
    "])\n",
    "\n",
    "\n",
    "cat_preprocessing = pipeline.Pipeline(steps = [\n",
    "    ('cat', impute.SimpleImputer(strategy = 'most_frequent'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стандартизуем вещественные признаки и кодируем категориальные: \n",
    "Вещественные признаки отличаются друг от друга по модулю значений, поэтому выполним их стандартизацию. Для использования категориальных признаков при построении модели их необходимо преобразовать в вещественные. Будем использовать one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preprocessing.steps.append(\n",
    "    ('num_scaler', preprocessing.StandardScaler())\n",
    ")\n",
    "\n",
    "\n",
    "cat_preprocessing.steps.append(\n",
    "    ('cat_encoder', preprocessing.OneHotEncoder(handle_unknown = 'ignore'))\n",
    ")\n",
    "\n",
    "\n",
    "# трансформер для заполнения пропусков и преобразования признаков  \n",
    "data_preprocessing = compose.ColumnTransformer(transformers = [\n",
    "    ('num_features', num_preprocessing, num_features),\n",
    "    ('cat_features', cat_preprocessing, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение baseline-моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы в данной задаче несбалансированны, поэтому добавим балансировку классов в модель. Остальные параметры оставим по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = pipeline.Pipeline(steps = [\n",
    "    ('preprocessing', data_preprocessing),\n",
    "    ('logistic', linear_model.LogisticRegression(class_weight = 'balanced', \n",
    "                                                 n_jobs = -1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество оценим с помощью кросс-валидации. В качетве вспомогательной метрики выбирем F1 меру.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, data, y, cv = 4, scoring = 'roc_auc'):\n",
    "    result = model_selection.cross_val_score(model, data,y, \n",
    "                               cv = cv, scoring = scoring,  \n",
    "                               n_jobs = -1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_skf = model_selection.StratifiedKFold(n_splits = 4, random_state = 1)\n",
    "\n",
    "roc_auc_score_logistic = cross_validation(logistic_regression, data_train, y_train, \n",
    "                               cv = cv_skf, scoring = 'roc_auc')\n",
    "\n",
    "f1_score_logistic = cross_validation(logistic_regression, data_train, y_train, \n",
    "                               cv = cv_skf, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: [0.6368168  0.65440807 0.65184991 0.63216506]  mean: 0.6438\n",
      " \n",
      "f1_score: [0.18686869 0.1894393  0.19091904 0.18297052]  mean: 0.1875\n"
     ]
    }
   ],
   "source": [
    "print('roc_auc_score: {}  mean: {}'.format(roc_auc_score_logistic, \n",
    "                                          round(roc_auc_score_logistic.mean(), 4)))\n",
    "print(' ')\n",
    "print('f1_score: {}  mean: {}'.format(f1_score_logistic, \n",
    "                                     round(f1_score_logistic.mean(), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Случайный лес:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = pipeline.Pipeline(steps = [\n",
    "    ('preprocessing', data_preprocessing), \n",
    "    ('forest', ensemble.RandomForestClassifier(max_depth = 5,\n",
    "                                               class_weight = 'balanced',\n",
    "                                               n_jobs = -1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_forest = cross_validation(random_forest, data_train, y_train, \n",
    "                               cv = cv_skf, scoring = 'roc_auc')\n",
    "\n",
    "f1_score_forest = cross_validation(random_forest, data_train, y_train, \n",
    "                               cv = cv_skf, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: [0.65845972 0.66509684 0.67088448 0.68177553]  mean: 0.6691\n",
      " \n",
      "f1_score: [0.19291887 0.20099119 0.20132743 0.19994724]  mean: 0.1988\n"
     ]
    }
   ],
   "source": [
    "print('roc_auc_score: {}  mean: {}'.format(roc_auc_score_forest, \n",
    "                                          round(roc_auc_score_forest.mean(), 4)))\n",
    "print(' ')\n",
    "print('f1_score: {}  mean: {}'.format(f1_score_forest, \n",
    "                                     round(f1_score_forest.mean(), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный бустинг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array in DataFrame with features name\n",
    "class DataForCatboost(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_features, cat_features):\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.data = None\n",
    "        self.size_data = 0\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        self.size_data = X.shape[0]\n",
    "        self.data = pd.DataFrame(columns = self.num_features + self.cat_features, \n",
    "                                data = X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.size_data == X.shape[0] or self.size_data == 0:\n",
    "            return self.data\n",
    "        else: \n",
    "            current_data = pd.DataFrame(columns = self.num_features + self.cat_features, \n",
    "                                data = X)\n",
    "            return current_data\n",
    "    \n",
    "    def fit_transform(self, X, y = None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocessing_for_boosting = pipeline.Pipeline(steps = [\n",
    "    ('cat_impute', impute.SimpleImputer(strategy = 'most_frequent'))\n",
    "])\n",
    "\n",
    "\n",
    "# трансформер для заполнения пропусков и преобразования вещественных признаков\n",
    "features_for_catboost = compose.ColumnTransformer(transformers = [\n",
    "    ('num_features', num_preprocessing, num_features),\n",
    "    ('cat_features', cat_preprocessing_for_boosting, cat_features)\n",
    "])\n",
    "\n",
    "\n",
    "# итоговый pipeline для предобработки данных\n",
    "all_features = pipeline.Pipeline(steps = [\n",
    "    ('feature', features_for_catboost), \n",
    "    ('data', DataForCatboost(num_features, cat_features))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catboost.CatBoostClassifier(n_estimators = 100,\n",
    "                                    max_depth = 5,\n",
    "                                    class_weights = [0.07, 0.93],\n",
    "                                    cat_features = cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_for_catboost(model, feature_transformer, data, y, cv, metrics):\n",
    "    result = []\n",
    "    for train_indices, test_indices in cv.split(data, y):\n",
    "        current_model = model\n",
    "        transformer = feature_transformer\n",
    "        # train data and target\n",
    "        data_train = transformer.fit_transform(data.iloc[train_indices])\n",
    "        y_train = y.iloc[train_indices]\n",
    "        # tets data and target\n",
    "        data_test = transformer.transform(data.iloc[test_indices])\n",
    "        y_test = y.iloc[test_indices]\n",
    "        \n",
    "        # fit on train data and predict on dat test\n",
    "        current_model.fit(data_train, y_train, verbose = False)\n",
    "        result.append(metrics(y_test, current_model.predict(data_test)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_catboost = cross_validation_for_catboost(model, all_features, data_train, y_train, \n",
    "                                                       cv = cv_skf, \n",
    "                                                       metrics = metrics.roc_auc_score)\n",
    "\n",
    "\n",
    "f1_score_catboost = cross_validation_for_catboost(model, all_features, data_train, y_train, \n",
    "                                                  cv = cv_skf, \n",
    "                                                  metrics = metrics.f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: [0.6291189416930446, 0.6460835403094992, 0.6308057691311255, 0.6374517461004876]  mean: 0.6359\n",
      " \n",
      "f1_score: [0.21364985163204747, 0.22847457627118647, 0.21440208536982733, 0.22117962466487937]  mean: 0.2194\n"
     ]
    }
   ],
   "source": [
    "print('roc_auc_score: {}  mean: {}'.format(roc_auc_score_catboost, \n",
    "                                          round(np.array(roc_auc_score_catboost).mean(), 4)))\n",
    "print(' ')\n",
    "print('f1_score: {}  mean: {}'.format(f1_score_catboost, \n",
    "                                     round(np.array(f1_score_catboost).mean(), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Целью** данной работы было построение baseline-моделей и оценка их качества. \n",
    "\n",
    "Были построены слудущие модели:\n",
    " - **Логистическая регрессия**:  ROC-AUC = 0.64\n",
    " - **Случайный лес**:  ROC-AUC = 0.67\n",
    " - **Градиентный бустинг** (catboost): ROC-AUC = 0.64\n",
    "\n",
    "При построении основного решения упор стоит сделать на модели, основанные на деревьях (случайный лес и градиентный бустинг), т к они показали лучшее качество.\n",
    "\n",
    "На всех моделях вспомогательня метрика качества **f1-мера** очень маленькая. Определяется **f1-мера** как среднее гармоническое точности и полноты, поэтому стоит отдельно посмотреть на эти них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся случайным лесом и оценим **точность** и **полноту** на кросс-валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_forest = cross_validation(random_forest, data_train, y_train, \n",
    "                               cv = cv_skf, scoring = 'precision')\n",
    "\n",
    "recall_forest = cross_validation(random_forest, data_train, y_train, \n",
    "                               cv = cv_skf, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision mean: 0.119\n",
      " \n",
      "recall mean: 0.6033\n"
     ]
    }
   ],
   "source": [
    "print('precision mean: {}'.format(round(precision_forest.mean(), 4)))\n",
    "print(' ')\n",
    "print('recall mean: {}'.format(round(recall_forest.mean(), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем очень низкое значение **точности**. Значит наш алгоритм совершает много ошибок **false positive**, т е происходит ложные срабатывания (относим объект к классу \"отток\", при его истином значении класса \"не отток\". \n",
    "\n",
    "В нашей задаче хуже совершать ошибки **false negative**, т е относить объект к классу \"не отток\" при его истинном значении класса \"отток\". Это приведет к потере клиентов, т к мы будем пропускать клиентов, которые хотят осуществить переход конкуренту и не сможем применить к ним методы для удержания."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
